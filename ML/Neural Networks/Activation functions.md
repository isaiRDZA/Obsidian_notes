Why does a neural network needs a Non-Linear Activation Function?
If you use a hidden layer with linear activation function, it's output is not going to be very different of a simple linear activation function.
[[Linear activation function]]
[[ReLU]]
[[Sigmoid]]
[[Softmax]]
[[tanh]]

![[Pasted image 20240504143652.png]]
