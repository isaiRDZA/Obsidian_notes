1. What is the "cache" used for in our implementation of forward propagation?
- [ ] We use it to pass variables computed during backward propagation to the corresponding forward step. It contains useful values for forward propagation to compute activations.
- [ ] It is used to keep track of the hyperparameters that we are searching over, to speed up computation.
- [ ] We use it to pass $Z$ computed during forward propagation to the corresponding backward propagation step. It contains useful values for backward propagation to compute derivatives.
- [ ] It is used to cache the intermediate values of the cost function during training.

2. During the backpropagation process, we use gradient descent to change the hyperparametes. True/False?
- [ ] False
- [ ] True

3. Considering the intermediate results below, which layers of a deep neural networks are they likely to belong.
 ![[Pasted image 20240515204113.png]]
- [ ] Later layers of the deep neural network.
- [ ] Input layer of the deep neural network.
- [ ] Middle layers of the deep neural network.
- [ ] Early layers of the deep network.

4. Vectorization allows you to compute 
