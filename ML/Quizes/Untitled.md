1.  Suppose a neural network contains a dense layer that connects one hidden layer $h$, with $n$ units to a following hidden layer $h_{i+1}$ units. How many parameters are needed for this connection?
- [ ] $n_{i}, n_{i+1}$
- [ ] $(n_{i}+1)n_{i+1}$
- [ ] $n_{i}$
- [ ] $n_{i}+n_{i+1}$
2. Why do we use non-linear activation functions (such as tanhtanh or relurelu) in neural networks?
- [ ] Without non-linear activation functions, the network would only be able to model linear functions of the data.
- [ ] To induce sparse connectivity in the network weights.
- [ ] 